# F1 Results Scraper - CI/CD 
# ====================================================

name: F1 Scraper CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 6 * * *'
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'

jobs:
  test:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.9', '3.10', '3.11']

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Test scraper functionality
      run: |
        python -c "
        try:
            from f1_scraper_pro import F1ResultsScraperPro
            scraper = F1ResultsScraperPro(year=2024, rate_limit=0.1)
            print('âœ… Scraper works correctly')
            print(f'âœ… Base URL: {scraper.base_url}')
        except Exception as e:
            print(f'âŒ Error: {e}')
            import traceback
            traceback.print_exc()
            exit(1)
        "

    - name: Test CLI
      run: |
        python f1_scraper_pro.py --help

  dry-run:
    runs-on: ubuntu-latest
    needs: test
    timeout-minutes: 10
    if: github.event_name != 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Test network connectivity
      run: |
        python -c "
        import requests
        try:
            response = requests.get('https://www.formula1.com', timeout=15)
            print(f'âœ… F1 website accessible: {response.status_code}')
        except Exception as e:
            print(f'âš ï¸ Network issue: {e}')
        "

  scrape:
    runs-on: ubuntu-latest
    needs: test
    timeout-minutes: 30
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run F1 scraper
      continue-on-error: true
      run: |
        mkdir -p ./data
        echo "ğŸï¸ Starting F1 scraping..."
        python f1_scraper_pro.py --year 2025 --output-dir ./data --summary --rate-limit 3.0 2>&1 | tee scraper_output.log

    - name: Check results
      run: |
        echo "ğŸ“Š Scraping results:"
        ls -la ./data/ || echo "No data directory"
        
        if [ -f "./data/f1_2025_results.csv" ]; then
          echo "âœ… CSV created: $(wc -l < ./data/f1_2025_results.csv) lines"
          echo "ğŸ“„ File size: $(du -h ./data/f1_2025_results.csv | cut -f1)"
        else
          echo "âš ï¸ No CSV file found"
        fi
        
        if [ -f "./data/f1_2025_results.json" ]; then
          echo "âœ… JSON file created"
        else
          echo "âš ï¸ No JSON file found"
        fi

    - name: Upload scraping results
      uses: actions/upload-artifact@v4  # Updated to v4
      if: always()
      with:
        name: f1-results-${{ github.run_id }}
        path: |
          ./data/
          scraper_output.log
        retention-days: 30
        if-no-files-found: warn

    - name: Scraping summary
      if: always()
      run: |
        echo ""
        echo "ğŸ F1 Scraping Summary"
        echo "======================"
        echo "Timestamp: $(date -u)"
        echo "Event Type: ${{ github.event_name }}"
        echo "Run ID: ${{ github.run_id }}"
        echo ""
        
        if [ -f "./data/f1_2025_results.csv" ]; then
          races_count=$(tail -n +2 ./data/f1_2025_results.csv 2>/dev/null | wc -l || echo "0")
          echo "âœ… SUCCESS: Scraped $races_count races"
          echo "ğŸ“ Results uploaded to artifacts"
        else
          echo "âŒ No results generated"
          echo "ğŸ“‹ This might be expected if the 2025 season hasn't started yet"
        fi

  docker:
    runs-on: ubuntu-latest
    needs: test
    timeout-minutes: 15
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: false
        tags: |
          f1-scraper:latest
          f1-scraper:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: Docker build summary
      run: |
        echo "âœ… Docker build completed successfully!"
        echo "ğŸ·ï¸  Tags: f1-scraper:latest, f1-scraper:${{ github.sha }}"
        echo "ğŸ“¦ Image is ready for deployment"

  notify:
    runs-on: ubuntu-latest
    needs: [test, dry-run, scrape, docker]
    if: always() && github.event_name == 'schedule'
    timeout-minutes: 5
    
    steps:
    - name: Workflow Summary
      run: |
        echo "ğŸï¸ F1 Scraper Workflow Summary"
        echo "=============================="
        echo "Test Job: ${{ needs.test.result }}"
        echo "Dry Run: ${{ needs.dry-run.result }}"
        echo "Scrape Job: ${{ needs.scrape.result }}"
        echo "Docker Job: ${{ needs.docker.result }}"
        echo ""
        
        if [ "${{ needs.scrape.result }}" = "success" ]; then
          echo "âœ… Scheduled scraping completed successfully"
        elif [ "${{ needs.scrape.result }}" = "failure" ]; then
          echo "âŒ Scheduled scraping failed - check logs"
        else
          echo "â„¹ï¸ Scraping was skipped or cancelled"
        fi